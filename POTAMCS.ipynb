{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnylut1X2Y5iIfXSb9tvfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ogutiann/6COSC019C-Cyber-Security/blob/main/POTAMCS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEI7PKHHe8Bw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install numpy torch scikit-learn deap pandas matplotlib seaborn pygad networkx\n",
        "!pip install --upgrade torch\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from deap import algorithms, base, creator, tools\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Configuration for large-scale deployment\n",
        "N_WORKERS = 5000\n",
        "N_TASKS = 10000\n",
        "CAPACITY = 3  # Max tasks per worker\n",
        "GPU_ACCELERATED = torch.cuda.is_available()\n",
        "\n",
        "if GPU_ACCELERATED:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"🚀 Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"⚠️ Using CPU - GPU recommended for large-scale simulation\")\n",
        "\n",
        "# Load and preprocess real-world datasets\n",
        "def load_real_world_data():\n",
        "    \"\"\"Load and preprocess NYC environmental datasets\"\"\"\n",
        "    # Load air quality data\n",
        "    try:\n",
        "        air_quality = pd.read_csv('air_quality.csv')\n",
        "        # Filter and preprocess\n",
        "        air_quality = air_quality[['geo_entity_name', 'data_valuemessage']]\n",
        "        air_quality = air_quality.groupby('geo_entity_name').mean().reset_index()\n",
        "        print(\"✅ Loaded air quality data with borough averages:\")\n",
        "        print(air_quality.head())\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Air quality data loading failed: {e}\")\n",
        "        air_quality = None\n",
        "\n",
        "    # Load climate data\n",
        "    try:\n",
        "        climate = pd.read_csv('climate.csv')\n",
        "        # Filter and preprocess\n",
        "        climate = climate[['time', 'temperature_2m']].copy()\n",
        "        climate['time'] = pd.to_datetime(climate['time'])\n",
        "        climate = climate.dropna()\n",
        "        print(\"✅ Loaded climate data with temperature readings:\")\n",
        "        print(climate.head())\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Climate data loading failed: {e}\")\n",
        "        climate = None\n",
        "\n",
        "    return air_quality, climate\n",
        "\n",
        "# Preload datasets\n",
        "AIR_QUALITY_DF, CLIMATE_DF = load_real_world_data()\n",
        "\n",
        "######################\n",
        "### 1. System Model ##\n",
        "######################\n",
        "class Worker:\n",
        "    def __init__(self, id, real_world=False):\n",
        "        self.id = id\n",
        "        self.real_world = real_world\n",
        "\n",
        "        if real_world:\n",
        "            # Use actual NYC borough characteristics\n",
        "            self.location = self.generate_nyc_location()\n",
        "            self.skill = self.generate_nyc_skills()\n",
        "            self.speed = self.generate_nyc_speed()\n",
        "        else:\n",
        "            self.location = np.array([np.random.uniform(0,100), np.random.uniform(0,100)])\n",
        "            self.skill = np.array([np.random.uniform(2,5), np.random.uniform(2,5)])\n",
        "            self.speed = np.random.uniform(0.5, 2)\n",
        "\n",
        "        self.weights = np.random.dirichlet(np.ones(3))\n",
        "        self.reputation = np.random.uniform(0.7, 0.95)\n",
        "        self.utility_history = []\n",
        "        self.assigned_tasks = []\n",
        "\n",
        "    def generate_nyc_location(self):\n",
        "        # NYC borough probabilities based on population density\n",
        "        borough_probs = [0.3, 0.3, 0.2, 0.15, 0.05]  # Manhattan, Brooklyn, Queens, Bronx, SI\n",
        "        borough = np.random.choice(5, p=borough_probs)\n",
        "\n",
        "        # Approximate centroids of NYC boroughs\n",
        "        centroids = {\n",
        "            0: [40.7831, -73.9712],  # Manhattan\n",
        "            1: [40.6782, -73.9442],  # Brooklyn\n",
        "            2: [40.7282, -73.7949],  # Queens\n",
        "            3: [40.8448, -73.8648],  # Bronx\n",
        "            4: [40.5795, -74.1502]   # Staten Island\n",
        "        }\n",
        "\n",
        "        # Add noise around centroid\n",
        "        lat = centroids[borough][0] + np.random.uniform(-0.05, 0.05)\n",
        "        lon = centroids[borough][1] + np.random.uniform(-0.05, 0.05)\n",
        "        return np.array([lat, lon])\n",
        "\n",
        "    def generate_nyc_skills(self):\n",
        "        # Skill levels correlate with borough socioeconomic factors\n",
        "        if self.location[0] > 40.75:  # Manhattan\n",
        "            return np.array([np.random.uniform(4, 5), np.random.uniform(4, 5)])\n",
        "        elif self.location[0] > 40.65:  # Brooklyn/Queens\n",
        "            return np.array([np.random.uniform(3.5, 4.5), np.random.uniform(3.5, 4.5)])\n",
        "        else:  # Bronx/Staten Island\n",
        "            return np.array([np.random.uniform(3, 4), np.random.uniform(3, 4)])\n",
        "\n",
        "    def generate_nyc_speed(self):\n",
        "        # Travel speeds vary by borough\n",
        "        if self.location[0] > 40.75:  # Manhattan (congested)\n",
        "            return np.random.uniform(0.5, 1.2)\n",
        "        elif self.location[0] > 40.65:  # Brooklyn/Queens\n",
        "            return np.random.uniform(1.0, 1.8)\n",
        "        else:  # Bronx/Staten Island\n",
        "            return np.random.uniform(1.5, 2.0)\n",
        "\n",
        "    def travel_time(self, task_location):\n",
        "        # Haversine distance for real-world locations\n",
        "        R = 6371  # Earth radius in km\n",
        "        lat1, lon1 = np.radians(self.location)\n",
        "        lat2, lon2 = np.radians(task_location)\n",
        "\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "\n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "        distance = R * c\n",
        "\n",
        "        traffic_noise = np.random.laplace(0, 0.1)\n",
        "        return max(0.1, distance / self.speed + traffic_noise)\n",
        "\n",
        "class Task:\n",
        "    def __init__(self, id, real_world=False):\n",
        "        self.id = id\n",
        "        self.real_world = real_world\n",
        "\n",
        "        if real_world and (AIR_QUALITY_DF is not None or CLIMATE_DF is not None):\n",
        "            self.location, self.reward, self.difficulty, self.required_skill = self.generate_real_task()\n",
        "        else:\n",
        "            self.location = np.array([np.random.uniform(0,100), np.random.uniform(0,100)])\n",
        "            self.reward = np.random.uniform(5,20)\n",
        "            self.difficulty = np.random.uniform(1,10)\n",
        "            self.required_skill = np.array([np.random.uniform(1,4), np.random.uniform(1,4)])\n",
        "\n",
        "        self.deadline = np.random.randint(10,60)\n",
        "\n",
        "    def generate_real_task(self):\n",
        "        \"\"\"Generate task based on real NYC environmental data\"\"\"\n",
        "        # Randomly choose task type (air quality or temperature)\n",
        "        task_type = np.random.choice([\"air_quality\", \"temperature\"])\n",
        "\n",
        "        # NYC borough centroids\n",
        "        borough_centroids = {\n",
        "            \"Manhattan\": [40.7831, -73.9712],\n",
        "            \"Brooklyn\": [40.6782, -73.9442],\n",
        "            \"Queens\": [40.7282, -73.7949],\n",
        "            \"Bronx\": [40.8448, -73.8648],\n",
        "            \"Staten Island\": [40.5795, -74.1502]\n",
        "        }\n",
        "\n",
        "        if task_type == \"air_quality\" and AIR_QUALITY_DF is not None:\n",
        "            # Select random borough\n",
        "            borough = np.random.choice(AIR_QUALITY_DF['geo_entity_name'])\n",
        "            b_data = AIR_QUALITY_DF[AIR_QUALITY_DF['geo_entity_name'] == borough].iloc[0]\n",
        "\n",
        "            # Set task parameters based on air quality data\n",
        "            location = np.array(borough_centroids.get(borough, [40.7128, -74.0060]))\n",
        "            benzene_level = b_data['data_valuemessage']\n",
        "\n",
        "            # Higher benzene levels = more difficult tasks\n",
        "            difficulty = np.clip(benzene_level/5, 1, 10)  # Scale to 1-10\n",
        "            reward = 10 + difficulty * 2  # Higher reward for difficult tasks\n",
        "            required_skill = np.array([3.0 + difficulty/3, 3.5 + difficulty/3])\n",
        "\n",
        "            return location, reward, difficulty, required_skill\n",
        "\n",
        "        else:  # Temperature task\n",
        "            if CLIMATE_DF is not None:\n",
        "                # Get random temperature reading\n",
        "                temp_row = CLIMATE_DF.sample(1).iloc[0]\n",
        "                temperature = temp_row['temperature_2m']\n",
        "            else:\n",
        "                temperature = np.random.uniform(-5, 35)  # NYC temperature range\n",
        "\n",
        "            # Select random borough\n",
        "            borough = np.random.choice(list(borough_centroids.keys()))\n",
        "            location = np.array(borough_centroids.get(borough, [40.7128, -74.0060]))\n",
        "\n",
        "            # Set task parameters based on temperature\n",
        "            difficulty = np.clip(abs(temperature-20)/5, 1, 10)  # More extreme temps = harder\n",
        "            reward = 8 + difficulty * 1.5\n",
        "            required_skill = np.array([2.5 + difficulty/4, 3.0 + difficulty/4])\n",
        "\n",
        "            return location, reward, difficulty, required_skill\n",
        "\n",
        "################################\n",
        "### 2. Deep-Quality Reputation ##\n",
        "################################\n",
        "class QualityVAE(nn.Module):\n",
        "    def __init__(self, input_dim=3, latent_dim=8):\n",
        "        super(QualityVAE, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.mu = nn.Linear(16, latent_dim)\n",
        "        self.logvar = nn.Linear(16, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.mu(h), self.logvar(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + 0.5 * KLD\n",
        "\n",
        "def compute_quality(features, vae_model):\n",
        "    with torch.no_grad():\n",
        "        recon, _, _ = vae_model(features)\n",
        "        error = torch.norm(features - recon, dim=1)\n",
        "    return 1 - error\n",
        "\n",
        "################################################\n",
        "### 3. Federated RL for Preference Optimization #\n",
        "################################################\n",
        "class LSTMPolicy(nn.Module):\n",
        "    def __init__(self, input_size=5, hidden_size=32, output_size=3):\n",
        "        super(LSTMPolicy, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.linear(x[:, -1, :])\n",
        "        return torch.softmax(x, dim=-1)\n",
        "\n",
        "def federated_average(models):\n",
        "    global_dict = {}\n",
        "    for key in models[0].state_dict().keys():\n",
        "        global_dict[key] = torch.stack([m.state_dict()[key] for m in models]).mean(0)\n",
        "    return global_dict\n",
        "\n",
        "def train_worker_rl(worker, tasks, epochs=20, batch_size=64):\n",
        "    model = LSTMPolicy().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    # Simulate historical data\n",
        "    n_samples = 1000\n",
        "    states = torch.randn(n_samples, 1, 5, device=device)\n",
        "    actions = torch.randint(0, 3, (n_samples,), device=device)\n",
        "    rewards = torch.rand(n_samples, device=device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(n_samples)\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_states = states[indices]\n",
        "            batch_actions = actions[indices]\n",
        "            batch_rewards = rewards[indices]\n",
        "\n",
        "            # Forward pass\n",
        "            action_probs = model(batch_states)\n",
        "            selected_probs = action_probs[torch.arange(len(batch_actions)), batch_actions]\n",
        "\n",
        "            # Policy gradient loss\n",
        "            loss = -torch.log(selected_probs) * batch_rewards\n",
        "            loss = loss.mean()\n",
        "\n",
        "            # Optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "#############################################\n",
        "### 4. Matching Algorithms Implementations ##\n",
        "#############################################\n",
        "def worker_utility(worker, task, beta=0.3):\n",
        "    tau = worker.travel_time(task.location)\n",
        "    skill_gap = np.mean(worker.skill - task.required_skill)\n",
        "    return (worker.weights[0] * (1/tau) +\n",
        "            worker.weights[1] * task.reward +\n",
        "            worker.weights[2] * skill_gap +\n",
        "            beta * worker.reputation)\n",
        "\n",
        "def task_utility(task, worker, zeta=0.3, kappa=0.7):\n",
        "    if np.any(worker.skill < task.required_skill):\n",
        "        return 0\n",
        "    tau = worker.travel_time(task.location)\n",
        "    return (kappa * (1/tau) + zeta * worker.reputation)\n",
        "\n",
        "# 4.1 RAGS (Our Proposed)\n",
        "def rags_matching(workers, tasks, capacity=CAPACITY, beta=0.3, zeta=0.3, kappa=0.7):\n",
        "    # Precompute utilities\n",
        "    w_utils = np.zeros((len(workers), len(tasks)))\n",
        "    t_utils = np.zeros((len(tasks), len(workers)))\n",
        "\n",
        "    for i, worker in enumerate(workers):\n",
        "        for j, task in enumerate(tasks):\n",
        "            w_utils[i, j] = worker_utility(worker, task, beta)\n",
        "            t_utils[j, i] = task_utility(task, worker, zeta, kappa)\n",
        "\n",
        "    # Preference lists\n",
        "    w_prefs = [np.argsort(-w_utils[i]) for i in range(len(workers))]\n",
        "    t_prefs = [np.argsort(-t_utils[j]) for j in range(len(tasks))]\n",
        "\n",
        "    # Initialize\n",
        "    proposals = np.zeros(len(workers), dtype=int)\n",
        "    worker_assignments = [[] for _ in range(len(workers))]\n",
        "    task_assignments = [[] for _ in range(len(tasks))]\n",
        "    unmatched = list(range(len(workers)))\n",
        "\n",
        "    # Matching loop\n",
        "    while unmatched:\n",
        "        i = unmatched.pop(0)\n",
        "        if proposals[i] >= len(tasks):\n",
        "            continue\n",
        "\n",
        "        j = w_prefs[i][proposals[i]]\n",
        "        proposals[i] += 1\n",
        "\n",
        "        if len(task_assignments[j]) < capacity:\n",
        "            worker_assignments[i].append(j)\n",
        "            task_assignments[j].append(i)\n",
        "        else:\n",
        "            # Find worst assigned worker\n",
        "            worst_idx = min(task_assignments[j], key=lambda x: t_utils[j, x])\n",
        "            if t_utils[j, i] > t_utils[j, worst_idx]:\n",
        "                # Replace worker\n",
        "                worker_assignments[worst_idx].remove(j)\n",
        "                task_assignments[j].remove(worst_idx)\n",
        "                unmatched.append(worst_idx)\n",
        "\n",
        "                worker_assignments[i].append(j)\n",
        "                task_assignments[j].append(i)\n",
        "            else:\n",
        "                unmatched.append(i)\n",
        "\n",
        "    return worker_assignments, task_assignments\n",
        "\n",
        "# 4.2 Classic Gale-Shapley\n",
        "def classic_gs(workers, tasks, capacity=CAPACITY):\n",
        "    # Static utilities\n",
        "    w_utils = np.zeros((len(workers), len(tasks)))\n",
        "    t_utils = np.zeros((len(tasks), len(workers)))\n",
        "\n",
        "    for i, worker in enumerate(workers):\n",
        "        for j, task in enumerate(tasks):\n",
        "            w_utils[i, j] = worker_utility(worker, task)\n",
        "            t_utils[j, i] = task_utility(task, worker)\n",
        "\n",
        "    # Same as RAGS but without dynamic adjustments\n",
        "    return rags_matching(workers, tasks, capacity)\n",
        "\n",
        "# 4.3 Random Allocation\n",
        "def random_matching(workers, tasks, capacity=CAPACITY):\n",
        "    worker_assignments = [[] for _ in range(len(workers))]\n",
        "    task_assignments = [[] for _ in range(len(tasks))]\n",
        "\n",
        "    # Shuffle all possible assignments\n",
        "    all_pairs = [(i, j) for i in range(len(workers)) for j in range(len(tasks))]\n",
        "    random.shuffle(all_pairs)\n",
        "\n",
        "    for i, j in all_pairs:\n",
        "        if (len(worker_assignments[i]) < capacity and\n",
        "            len(task_assignments[j]) < capacity and\n",
        "            np.all(workers[i].skill >= tasks[j].required_skill)):\n",
        "            worker_assignments[i].append(j)\n",
        "            task_assignments[j].append(i)\n",
        "\n",
        "    return worker_assignments, task_assignments\n",
        "\n",
        "# 4.4 FML (Federated Matching without RL)\n",
        "def fml_matching(workers, tasks, capacity=CAPACITY):\n",
        "    # Average weights across workers\n",
        "    avg_weights = np.mean([w.weights for w in workers], axis=0)\n",
        "\n",
        "    # Override worker preferences with global average\n",
        "    for worker in workers:\n",
        "        worker.weights = avg_weights.copy()\n",
        "\n",
        "    return classic_gs(workers, tasks, capacity)\n",
        "\n",
        "############################\n",
        "### 5. Evaluation Metrics ##\n",
        "############################\n",
        "def calculate_satisfaction(workers, tasks, assignments):\n",
        "    satisfaction = []\n",
        "    for i, worker in enumerate(workers):\n",
        "        utils = [worker_utility(worker, tasks[j]) for j in assignments[i]]\n",
        "        satisfaction.append(np.mean(utils) if utils else 0)\n",
        "    return np.mean(satisfaction)\n",
        "\n",
        "def calculate_task_coverage(task_assignments):\n",
        "    return sum(len(a) > 0 for a in task_assignments) / len(task_assignments)\n",
        "\n",
        "def calculate_quality(workers, tasks, assignments):\n",
        "    # Simulate data submissions with noise\n",
        "    qualities = []\n",
        "    for i, worker in enumerate(workers):\n",
        "        for task_id in assignments[i]:\n",
        "            task = tasks[task_id]\n",
        "            # Quality depends on worker skill and task difficulty\n",
        "            base_quality = np.mean(worker.skill - task.required_skill) / 5\n",
        "            noise = np.random.normal(0, 0.2 * (1 - worker.reputation))\n",
        "            qualities.append(max(0, min(1, base_quality + noise)))\n",
        "    return np.mean(qualities) if qualities else 0\n",
        "\n",
        "def check_stability(workers, tasks, worker_assignments, task_assignments):\n",
        "    blocking_pairs = 0\n",
        "    sample_size = min(1000, len(workers))\n",
        "\n",
        "    for i in np.random.choice(len(workers), sample_size, replace=False):\n",
        "        worker = workers[i]\n",
        "        current_utils = [worker_utility(worker, tasks[j]) for j in worker_assignments[i]]\n",
        "        current_min = min(current_utils) if current_utils else -np.inf\n",
        "\n",
        "        for j in np.random.choice(len(tasks), min(100, len(tasks)), replace=False):\n",
        "            if j in worker_assignments[i]:\n",
        "                continue\n",
        "\n",
        "            # Worker prefers this task\n",
        "            w_util = worker_utility(worker, tasks[j])\n",
        "            if w_util <= current_min:\n",
        "                continue\n",
        "\n",
        "            # Task prefers this worker\n",
        "            t_util = task_utility(tasks[j], worker)\n",
        "            current_workers = task_assignments[j]\n",
        "            if not current_workers:\n",
        "                blocking_pairs += 1\n",
        "                continue\n",
        "\n",
        "            min_current_util = min(task_utility(tasks[j], workers[k]) for k in current_workers)\n",
        "            if t_util > min_current_util:\n",
        "                blocking_pairs += 1\n",
        "\n",
        "    return 1 - (blocking_pairs / (sample_size * 100))\n",
        "\n",
        "########################\n",
        "### 6. Main Simulation #\n",
        "########################\n",
        "def run_full_simulation(real_world=False):\n",
        "    print(f\"🚀 Starting {'real-world' if real_world else 'synthetic'} simulation \"\n",
        "          f\"with {N_WORKERS} workers and {N_TASKS} tasks\")\n",
        "\n",
        "    results = []\n",
        "    methods = {\n",
        "        \"POTA (Proposed)\": rags_matching,\n",
        "        \"Classic GS\": classic_gs,\n",
        "        \"Random Allocation\": random_matching,\n",
        "        \"FML\": fml_matching\n",
        "    }\n",
        "\n",
        "    # Generate workers and tasks\n",
        "    print(\"Generating workers and tasks...\")\n",
        "    workers = [Worker(i, real_world=real_world) for i in tqdm(range(N_WORKERS))]\n",
        "    tasks = [Task(j, real_world=real_world) for j in tqdm(range(N_TASKS))]\n",
        "\n",
        "    # Train DQRS model\n",
        "    print(\"Training DQRS VAE...\")\n",
        "    vae_model = QualityVAE().to(device)\n",
        "    optimizer = optim.Adam(vae_model.parameters(), lr=0.001)\n",
        "\n",
        "    # Generate synthetic quality features\n",
        "    features = torch.randn(100000, 3, device=device)\n",
        "    for epoch in tqdm(range(50)):\n",
        "        recon, mu, logvar = vae_model(features)\n",
        "        loss = vae_loss(recon, features, mu, logvar)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Federated RL Training\n",
        "    print(\"Training federated RL agents...\")\n",
        "    worker_models = []\n",
        "    batch_size = min(100, len(workers))\n",
        "\n",
        "    for i in tqdm(range(0, len(workers), batch_size)):\n",
        "        batch = workers[i:i+batch_size]\n",
        "        batch_models = [train_worker_rl(w, tasks) for w in batch]\n",
        "        worker_models.extend(batch_models)\n",
        "\n",
        "        # Clear memory\n",
        "        if GPU_ACCELERATED:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Apply federated averaging\n",
        "    print(\"Applying federated averaging...\")\n",
        "    global_weights = federated_average(worker_models)\n",
        "    for i, model in enumerate(worker_models):\n",
        "        model.load_state_dict(global_weights)\n",
        "        workers[i].weights = model(torch.randn(1, 1, 5, device=device)).cpu().detach().numpy()[0]\n",
        "\n",
        "    # Run all matching algorithms\n",
        "    for method_name, matching_fn in methods.items():\n",
        "        print(f\"\\n🔍 Running {method_name}...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Reset assignments\n",
        "        for w in workers:\n",
        "            w.assigned_tasks = []\n",
        "\n",
        "        # Run matching\n",
        "        worker_assignments, task_assignments = matching_fn(workers, tasks)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        satisfaction = calculate_satisfaction(workers, tasks, worker_assignments)\n",
        "        coverage = calculate_task_coverage(task_assignments)\n",
        "        quality = calculate_quality(workers, tasks, worker_assignments)\n",
        "        stability = check_stability(workers, tasks, worker_assignments, task_assignments)\n",
        "\n",
        "        # Record results\n",
        "        results.append({\n",
        "            \"Method\": method_name,\n",
        "            \"Satisfaction\": satisfaction,\n",
        "            \"Task Coverage\": coverage,\n",
        "            \"Data Quality\": quality,\n",
        "            \"Stability\": stability,\n",
        "            \"Time (s)\": elapsed,\n",
        "            \"Scenario\": \"Real-world\" if real_world else \"Synthetic\"\n",
        "        })\n",
        "\n",
        "        print(f\"  Satisfaction: {satisfaction:.3f} | Coverage: {coverage:.1%} | \"\n",
        "              f\"Quality: {quality:.3f} | Stability: {stability:.1%} | Time: {elapsed:.1f}s\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_parameter_sensitivity():\n",
        "    print(\"🔬 Running parameter sensitivity analysis...\")\n",
        "    results = []\n",
        "    workers = [Worker(i) for i in range(1000)]  # Smaller scale for sensitivity\n",
        "    tasks = [Task(j) for j in range(2000)]\n",
        "\n",
        "    # Test beta (reputation weight)\n",
        "    for beta in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "        start_time = time.time()\n",
        "        worker_assignments, task_assignments = rags_matching(workers, tasks, beta=beta)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        satisfaction = calculate_satisfaction(workers, tasks, worker_assignments)\n",
        "        quality = calculate_quality(workers, tasks, worker_assignments)\n",
        "\n",
        "        results.append({\n",
        "            \"Parameter\": \"β\",\n",
        "            \"Value\": beta,\n",
        "            \"Satisfaction\": satisfaction,\n",
        "            \"Quality\": quality,\n",
        "            \"Time (s)\": elapsed\n",
        "        })\n",
        "\n",
        "    # Test learning rate (eta)\n",
        "    for lr in [0.001, 0.005, 0.01, 0.05, 0.1]:\n",
        "        # Retrain RL with different learning rate\n",
        "        worker_models = []\n",
        "        for w in workers:\n",
        "            model = train_worker_rl(w, tasks, lr=lr)\n",
        "            worker_models.append(model)\n",
        "\n",
        "        # Apply federated averaging\n",
        "        global_weights = federated_average(worker_models)\n",
        "        for i, model in enumerate(worker_models):\n",
        "            model.load_state_dict(global_weights)\n",
        "            workers[i].weights = model(torch.randn(1, 1, 5)).detach().numpy()[0]\n",
        "\n",
        "        # Run matching\n",
        "        start_time = time.time()\n",
        "        worker_assignments, task_assignments = rags_matching(workers, tasks)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        satisfaction = calculate_satisfaction(workers, tasks, worker_assignments)\n",
        "        quality = calculate_quality(workers, tasks, worker_assignments)\n",
        "\n",
        "        results.append({\n",
        "            \"Parameter\": \"η\",\n",
        "            \"Value\": lr,\n",
        "            \"Satisfaction\": satisfaction,\n",
        "            \"Quality\": quality,\n",
        "            \"Time (s)\": elapsed\n",
        "        })\n",
        "\n",
        "    # Test alpha (reputation decay)\n",
        "    for alpha in [0.6, 0.7, 0.8, 0.9, 0.95]:\n",
        "        # Modify reputation system (simplified)\n",
        "        original_reps = [w.reputation for w in workers]\n",
        "        for w in workers:\n",
        "            w.reputation = alpha * w.reputation  # Simulate decay effect\n",
        "\n",
        "        start_time = time.time()\n",
        "        worker_assignments, task_assignments = rags_matching(workers, tasks)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        satisfaction = calculate_satisfaction(workers, tasks, worker_assignments)\n",
        "        quality = calculate_quality(workers, tasks, worker_assignments)\n",
        "\n",
        "        results.append({\n",
        "            \"Parameter\": \"α\",\n",
        "            \"Value\": alpha,\n",
        "            \"Satisfaction\": satisfaction,\n",
        "            \"Quality\": quality,\n",
        "            \"Time (s)\": elapsed\n",
        "        })\n",
        "\n",
        "        # Restore reputations\n",
        "        for w, rep in zip(workers, original_reps):\n",
        "            w.reputation = rep\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def plot_results(results_df, sensitivity_df, scenario):\n",
        "    # Comparative results\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Satisfaction and Quality\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.barplot(x=\"Method\", y=\"Satisfaction\", data=results_df)\n",
        "    plt.title(f\"Worker Satisfaction ({scenario})\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.barplot(x=\"Method\", y=\"Data Quality\", data=results_df)\n",
        "    plt.title(f\"Data Quality Score ({scenario})\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Coverage and Stability\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.barplot(x=\"Method\", y=\"Task Coverage\", data=results_df)\n",
        "    plt.title(f\"Task Coverage ({scenario})\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.barplot(x=\"Method\", y=\"Stability\", data=results_df)\n",
        "    plt.title(f\"Matching Stability ({scenario})\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"pota_metrics_{scenario.lower().replace(' ', '_')}.png\")\n",
        "\n",
        "    # Sensitivity analysis\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for i, param in enumerate(['β', 'η', 'α']):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        param_df = sensitivity_df[sensitivity_df['Parameter'] == param]\n",
        "        plt.plot(param_df['Value'], param_df['Satisfaction'], 'o-', label='Satisfaction')\n",
        "        plt.plot(param_df['Value'], param_df['Quality'], 's-', label='Quality')\n",
        "        plt.title(f\"Sensitivity to {param}\")\n",
        "        plt.xlabel(\"Parameter Value\")\n",
        "        plt.ylabel(\"Metric Value\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"parameter_sensitivity.png\")\n",
        "    print(\"\\n📊 Visualizations saved\")\n",
        "\n",
        "# Run the simulations\n",
        "if __name__ == \"__main__\":\n",
        "    # Run synthetic scenario\n",
        "    synthetic_results = run_full_simulation(real_world=False)\n",
        "    synthetic_df = pd.DataFrame(synthetic_results)\n",
        "\n",
        "    # Run real-world scenario\n",
        "    realworld_results = run_full_simulation(real_world=True)\n",
        "    realworld_df = pd.DataFrame(realworld_results)\n",
        "\n",
        "    # Run sensitivity analysis\n",
        "    sensitivity_df = run_parameter_sensitivity()\n",
        "\n",
        "    # Combine and save results\n",
        "    results_df = pd.concat([synthetic_df, realworld_df])\n",
        "    results_df.to_csv(\"pota_results.csv\", index=False)\n",
        "    sensitivity_df.to_csv(\"parameter_sensitivity.csv\", index=False)\n",
        "    print(\"\\n✅ Results saved to pota_results.csv and parameter_sensitivity.csv\")\n",
        "\n",
        "    # Visualize results\n",
        "    plot_results(synthetic_df, sensitivity_df, \"Synthetic\")\n",
        "    plot_results(realworld_df, sensitivity_df, \"Real-world\")\n",
        "\n",
        "    # Display final results\n",
        "    print(\"\\n⭐ Final Results ⭐\")\n",
        "    print(results_df.groupby(['Scenario', 'Method']).mean())"
      ]
    }
  ]
}